---
marp: true
theme: default
paginate: true
backgroundColor: #fff
backgroundImage: url('https://marp.app/assets/hero-background.svg')
style: |
  section {
    font-size: 28px;
    font-family: 'PingFang SC', 'Hiragino Sans GB', 'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;
  }
  h1 {
    color: #FF6B35;
    font-family: 'PingFang SC', 'Hiragino Sans GB', 'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;
  }
  h2 {
    color: #004E98;
    font-family: 'PingFang SC', 'Hiragino Sans GB', 'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;
  }
  h3 {
    font-family: 'PingFang SC', 'Hiragino Sans GB', 'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;
  }
  p, li, td, th {
    font-family: 'PingFang SC', 'Hiragino Sans GB', 'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif;
  }
  code {
    font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', 'Courier New', monospace;
  }
  .columns {
    display: grid;
    grid-template-columns: repeat(2, minmax(0, 1fr));
    gap: 1rem;
  }
---

<!-- _class: lead -->

# MiniMax M2 æ¨¡å‹é€‚é… AWS Trainium2

## æŠ€æœ¯æ–¹æ¡ˆä¸å®æ–½æŠ¥å‘Š

**ä» Qwen3 MoE åˆ° MiniMax M2 (230B)**

---

## ğŸ“‹ è®®ç¨‹

1. **é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡**
2. **æ¨¡å‹æ¶æ„å¯¹æ¯”åˆ†æ**
3. **æŠ€æœ¯å®æ–½è·¯çº¿**
4. **å…³é”®é—®é¢˜æ·±åº¦å‰–æ**
5. **æ€§èƒ½å½±å“è¯„ä¼°**
6. **æˆæœå±•ç¤º**
7. **ä¼˜åŒ–å»ºè®®ä¸å±•æœ›**

---

<!-- _class: lead -->

# 1. é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

---

## é¡¹ç›®æ¦‚è¿°

<div class="columns">
<div>

### ğŸ¯ ç›®æ ‡
- **è¿ç§»**: GPU â†’ Trainium2
- **æ¨¡å‹**: MiniMax M2 (230B)
- **æ¶æ„**: 256 Experts MoE
- **è§„æ¨¡**: tp_degree=64

</div>
<div>

### âš¡ æŒ‘æˆ˜
- âŒ DGEç¼–è¯‘é™åˆ¶
- âŒ æ¶æ„å·®å¼‚å·¨å¤§
- âŒ ç‰ˆæœ¬å…¼å®¹æ€§
- âŒ ç²¾åº¦æŸå¤±é£é™©

</div>
</div>

---

## æ¨¡å‹è§„æ¨¡å¯¹æ¯”

| ç»´åº¦ | Qwen3-30B | MiniMax M2 | å¢é•¿ |
|------|-----------|-----------|------|
| **å‚æ•°é‡** | 30B | 230B | **+667%** |
| **ä¸“å®¶æ•°** | 128 | 256 | **+100%** |
| **å±‚æ•°** | 32 | 62 | **+94%** |
| **éšè—ç»´åº¦** | 4096 | 6144 | **+50%** |
| **TP Degree** | 32 | 64 | **+100%** |

---

<!-- _class: lead -->

# 2. æ¨¡å‹æ¶æ„å¯¹æ¯”åˆ†æ

---

## å…³é”®æ¶æ„å·®å¼‚

### ğŸ”´ é—®é¢˜1: Intermediate Size

```
Qwen3:    14336 / 32 = 448 âœ… (>= 32)
MiniMax:   1536 / 64 =  24 âŒ (< 32)
                           â†“
              è§¦å‘ DGE ç¼–è¯‘é”™è¯¯
```

### ğŸ”´ é—®é¢˜2: QK Normalization

```python
# Qwen3: Shared norm
q_norm: [128]           # æ‰€æœ‰headså…±äº«

# MiniMax M2: Per-head norm
q_norm: [6144] = [48Ã—128]  # æ¯ä¸ªheadç‹¬ç«‹
```

---

## æ¶æ„å¯¹æ¯”å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Qwen3 MoE (30B)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention: 32 heads â†’ Shared QK Norm               â”‚
â”‚ MoE: 128 experts â†’ intermediate=14336              â”‚
â”‚ TP=32 â†’ 14336/32=448 âœ…                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                MiniMax M2 (230B)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention: 48 heads â†’ Per-head QK Norm âš ï¸          â”‚
â”‚ MoE: 256 experts â†’ intermediate=1536               â”‚
â”‚ TP=64 â†’ 1536/64=24 âŒ DGEé™åˆ¶                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<!-- _class: lead -->

# 3. æŠ€æœ¯å®æ–½è·¯çº¿

---

## æ•´ä½“æµç¨‹

```
1ï¸âƒ£ åˆ›å»ºæ¨¡å‹æ–‡ä»¶ç»“æ„
   â””â”€ modeling_minimax_m2.py
   â””â”€ configuration_minimax_m2.py
   â””â”€ generation_demo.py

2ï¸âƒ£ é…ç½®Neuronå‚æ•°
   â””â”€ tp_degree=64
   â””â”€ blockwise_matmul_config

3ï¸âƒ£ è§£å†³DGEç¼–è¯‘é”™è¯¯ â­
   â””â”€ å‘ç°é…ç½®ä¼ æ’­å¤±è´¥
   â””â”€ moe.py â†’ moe_v2.py

4ï¸âƒ£ è§£å†³æƒé‡åŠ è½½é”™è¯¯ â­
   â””â”€ QK normå½¢çŠ¶è½¬æ¢
   â””â”€ RouterConfig dtype

5ï¸âƒ£ æˆåŠŸè¿è¡Œ âœ…
```

---

## æ–‡ä»¶ä¿®æ”¹æ¸…å•

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | é‡è¦æ€§ |
|------|---------|--------|
| **modeling_minimax_m2.py** | MoEåˆå§‹åŒ–, QK normè½¬æ¢ | â­â­â­â­â­ |
| **config.py** | RouterConfig dtypeè½¬æ¢ | â­â­â­â­ |
| **hf_adapter.py** | GenerationMixinç»§æ‰¿ | â­â­â­â­ |
| **modeling_minimax_m2_gpu.py** | transformerså…¼å®¹ | â­â­â­ |
| **generation_demo.py** | é…ç½®å‚æ•° | â­â­ |

---

<!-- _class: lead -->

# 4. å…³é”®é—®é¢˜æ·±åº¦å‰–æ

---

## ğŸ”¥ é—®é¢˜1: DGEç¼–è¯‘é”™è¯¯

### é”™è¯¯ä¿¡æ¯
```
[NLA001] Unhandled exception with message:
tensorizer(output tensor: float32<24 x 1536> $237367[block_idx_64])
Instruction DMACopy I-237367-0: Invalid Shape for Scalar DGE!
```

### æ ¹æœ¬åŸå› 
- **DGEè¦æ±‚**: `intermediate_size / tp_degree >= 32`
- **å®é™…æƒ…å†µ**: `1536 / 64 = 24 < 32` âŒ
- **è§¦å‘æ¡ä»¶**: ä½¿ç”¨NKI kernelçš„blockwise matmul

---

## è§£å†³æ–¹æ¡ˆ: use_torch_block_wise

### âŒ é”™è¯¯é…ç½®ï¼ˆæœªç”Ÿæ•ˆï¼‰

```python
# é…ç½®çœ‹èµ·æ¥æ­£ç¡®
neuron_config = MoENeuronConfig(
    blockwise_matmul_config={
        'use_torch_block_wise': True
    }
)

# ä½†å®é™…ä¸Š...
expert_mlps.blockwise_matmul_config.use_torch_block_wise = False âŒ
```

**ä¸ºä»€ä¹ˆï¼Ÿ** ä½¿ç”¨äº†é”™è¯¯çš„ `moe.py` è€Œé `moe_v2.py`

---

## æ ¹å› : moe vs moe_v2

<div class="columns">
<div>

### âŒ moe.py (æ—§ç‰ˆ)
```python
# ExpertMLPs
def __init__(
    self,
    use_torch_block_wise=False,  # é»˜è®¤False
    ...
):
    # å‚æ•°è¢«è¦†ç›–
```

**é—®é¢˜**: é…ç½®å¯¹è±¡æœªä¼ é€’ï¼Œä½¿ç”¨é»˜è®¤å€¼

</div>
<div>

### âœ… moe_v2.py (æ–°ç‰ˆ)
```python
# ExpertMLPsV2
def __init__(
    self,
    blockwise_matmul_config,  # å¯¹è±¡
    ...
):
    self.config = blockwise_matmul_config
```

**æ­£ç¡®**: ç›´æ¥ä¼ é€’é…ç½®å¯¹è±¡

</div>
</div>

---

## ä¿®å¤ä»£ç 

```python
# âŒ é”™è¯¯
from neuronx_distributed_inference.modules.moe import initialize_moe_module

self.mlp = initialize_moe_module(
    config=config,
    num_experts=...,  # æ—§API
    top_k=...,
)

# âœ… æ­£ç¡®
from neuronx_distributed_inference.modules.moe_v2 import initialize_moe_module

self.mlp = initialize_moe_module(config=config)  # æ–°API
```

---

## âœ… é—®é¢˜2: QK Norm æ­£ç¡®å®ç° (å·²ä¿®å¤)

### GPU ç‰ˆæœ¬è¡Œä¸ºåˆ†æ

```python
# MiniMax M2 çš„ qk_norm åœ¨ projection ä¹‹åã€reshape ä¹‹å‰åº”ç”¨
self.q_norm = RMSNorm(num_heads * head_dim)  # [6144]
self.k_norm = RMSNorm(num_kv_heads * head_dim)  # [1024]

def forward(...):
    Q = self.q_proj(hidden_states)  # [B, S, 6144]
    K = self.k_proj(hidden_states)  # [B, S, 1024]

    Q = self.q_norm(Q)  # åœ¨å®Œæ•´ç»´åº¦ä¸Š RMSNorm
    K = self.k_norm(K)

    Q = Q.reshape(B, S, 48, 128)  # ç„¶åæ‰ reshape
```

### Neuron æ­£ç¡®å®ç°

```python
class NeuronMiniMaxM2Attention:
    def __init__(self, config):
        # åˆ›å»ºå®Œæ•´ç»´åº¦çš„ norm
        self.q_norm = RMSNorm(num_heads * head_dim)
        self.k_norm = RMSNorm(num_kv_heads * head_dim)

    def prep_qkv_tensors(self, ...):
        Q, K, V = self.get_qkv_proj()(...)

        # åœ¨ reshape ä¹‹å‰åº”ç”¨ qk_norm
        Q = self.q_norm(Q)
        K = self.k_norm(K)

        # ç„¶åæ‰ move_heads_front
        Q = move_heads_front(Q, ...)
```

### âœ… ç»“æœ: ä¸ GPU å®Œå…¨ä¸€è‡´

---

## âœ… é—®é¢˜3: Router sigmoid å’Œ e_score_correction_bias (æ–°å¢)

### é—®é¢˜å‘ç°

```python
# GPU ç‰ˆæœ¬ä½¿ç”¨ sigmoidï¼Œè€Œé softmax
routing_weights = torch.sigmoid(router_logits)

# e_score_correction_bias ä¸æ˜¯å…¨é›¶ï¼(èŒƒå›´ 4.7 ~ 8.7)
scores_for_choice = routing_weights + self.e_score_correction_bias
```

### è§£å†³æ–¹æ¡ˆ

```python
# 1. é…ç½® router_config ä½¿ç”¨ sigmoid
neuron_config = MoENeuronConfig(
    router_config={'act_fn': 'sigmoid'},
    ...
)

# 2. åˆ›å»º RouterTopKWithBias ç±»
class RouterTopKWithBias(RouterTopK):
    def __init__(self, num_experts, ...):
        self.register_buffer("e_score_correction_bias", ...)

    def forward(self, hidden_states):
        affinities = self.apply_activation_fn(logits)  # sigmoid
        scores = affinities + self.e_score_correction_bias
        _, expert_index = torch.topk(scores, self.top_k)
        return logits, affinities, expert_index
```

---

## ğŸ”¥ é—®é¢˜4: transformersç‰ˆæœ¬å†²çª

### ç‰ˆæœ¬çŸ©é˜µ

| ç‰ˆæœ¬ | masking_utils | GenerationMixin | FP8é‡åŒ–æ£€æŸ¥ |
|------|--------------|----------------|------------|
| 4.51.3 | âŒ | âœ… | âŒ |
| 4.52-4.49 | âœ… | âœ… | âŒ |
| 4.50+ | âœ… | âŒ | âœ… |
| **4.57.1** | âœ… | âŒ | âœ… |

**æ— æ³•é™çº§ â†’ éœ€è¦å…¼å®¹å±‚**

---

## è§£å†³æ–¹æ¡ˆ: å¤šé‡ç»§æ‰¿

```python
# âŒ é”™è¯¯ (4.50+ä¼šå¤±è´¥)
class HuggingFaceGenerationAdapter(PreTrainedModel):
    def generate(self, ...):
        return super().generate(...)  # AttributeError!

# âœ… æ­£ç¡®
from transformers.generation import GenerationMixin

class HuggingFaceGenerationAdapter(GenerationMixin, PreTrainedModel):
    def __init__(self, model, ...):
        PreTrainedModel.__init__(self, hf_config)
        # GenerationMixinåœ¨å‰ï¼Œç¡®ä¿generate()å¯ç”¨
```

---

<!-- _class: lead -->

# 5. æ€§èƒ½å½±å“è¯„ä¼°

---

## å½±å“å› ç´ åˆ†æï¼ˆæ›´æ–°ï¼‰

```
å·²ä¿®å¤çš„é—®é¢˜:
  âœ… QK Norm: æ­£ç¡®å®ç°åœ¨å®Œæ•´ç»´åº¦ä¸Šåº”ç”¨
  âœ… Router: sigmoid + e_score_correction_bias
  âœ… FP8ç²¾åº¦: scale å‚æ•°æ­£ç¡®è½¬æ¢
  âœ… o_proj: ç§»é™¤é‡å¤é‡å‘½å

å‰©ä½™å½±å“:
  âš ï¸ PyTorch blockwiseæ€§èƒ½ (vs NKI kernel)
```

### âœ… ä¸»è¦é—®é¢˜å·²ä¿®å¤

- **QK Norm**: ä¸ GPU ç‰ˆæœ¬å®Œå…¨ä¸€è‡´
- **Router**: sigmoid + bias æ­£ç¡®æ”¯æŒ
- **ç²¾åº¦**: FP8 scale æ­£ç¡®å¤„ç†

---

## GPU vs Neuron å®ç°å¯¹æ¯”

| ç»„ä»¶ | GPU ç‰ˆæœ¬ | Neuron ç‰ˆæœ¬ | çŠ¶æ€ |
|------|---------|------------|------|
| **QK Norm** | åœ¨ `[num_heads*head_dim]` ä¸Š | åŒå·¦ | âœ… |
| **Router æ¿€æ´»** | sigmoid | sigmoid | âœ… |
| **e_score_correction_bias** | æ”¯æŒ | æ”¯æŒ | âœ… |
| **Partial Rotary** | rotary_dim=64 | åŒå·¦ | âœ… |
| **RMSNorm** | MiniMaxM2RMSNorm | CustomRMSNorm | âœ… |

### âœ… æ‰€æœ‰å…³é”®ç»„ä»¶å·²æ­£ç¡®å®ç°

---

## âœ… FP8é‡åŒ–é—®é¢˜å·²è§£å†³

### å®Œæ•´è§£å†³æ–¹æ¡ˆ

```python
# 1. æ¢å¤ quantization_config (ä¿ç•™åœ¨ config.json)
# 2. æ™ºèƒ½ç»•è¿‡ transformers FP8 GPU æ£€æŸ¥
# 3. è½¬æ¢ 47,864 ä¸ª scale å‚æ•°
# 4. å¯ç”¨ Neuron FP8 é‡åŒ–å†…æ ¸
```

### é…ç½®æ›´æ–°

```python
neuron_config = MoENeuronConfig(
    tp_degree=64,
    quantized_mlp_kernel_enabled=True,  # âœ… å¯ç”¨FP8
    modules_to_not_convert=["lm_head"],
    blockwise_matmul_config={'use_torch_block_wise': True},
)
```

```bash
export XLA_HANDLE_SPECIAL_SCALAR=1  # âœ… å¿…éœ€
```

### FP8 æ€§èƒ½ä¼˜åŠ¿

- **å†…å­˜**: -50% vs BF16
- **é€Ÿåº¦**: +1.5-2x (ç¡¬ä»¶åŠ é€Ÿ)
- **ç²¾åº¦**: <1% æŸå¤± (per-channel scale)

---

## PyTorch vs NKI Kernel

### æ€§èƒ½å¯¹æ¯”ï¼ˆä¼°ç®—ï¼‰

```
NKI Kernel (DGEä¼˜åŒ–):
  âœ… ç¡¬ä»¶çº§ä¼˜åŒ–
  âœ… å†…å­˜å¸ƒå±€ä¼˜åŒ–
  âœ… ä½å»¶è¿Ÿ

PyTorchå®ç° (use_torch_block_wise):
  âŒ é€šç”¨å®ç°
  âŒ æœªä¼˜åŒ–å†…å­˜
  âš ï¸  å»¶è¿Ÿå¢åŠ 20-40%
```

**Expert MLPæ˜¯MoEçš„æ€§èƒ½ç“¶é¢ˆ**

---

## æ€§èƒ½åŸºå‡†

| æŒ‡æ ‡ | Qwen3-30B | MiniMax M2 | å·®è· |
|------|----------|-----------|------|
| **ç¼–è¯‘æ—¶é—´** | ~40 min | ~50 min | +25% |
| **åŠ è½½æ—¶é—´** | ~180 s | ~233 s | +29% |
| **Warmup** | ~2 s | ~2.4 s | +20% |
| **Token/s** | ~15 | ~10 | -33% |
| **è¾“å‡ºè´¨é‡** | âœ… æ­£å¸¸ | âŒ å¾…ä¼˜åŒ– | ä¸¥é‡ |

---

<!-- _class: lead -->

# 6. æˆæœå±•ç¤º

---

## âœ… å®Œæ•´æˆæœ

<div class="columns">
<div>

### æŠ€æœ¯çªç ´
- âœ… **DGEé™åˆ¶**: ç»•è¿‡ç¼–è¯‘éšœç¢
- âœ… **é…ç½®ä¼ æ’­**: è¯†åˆ«moe_v2é—®é¢˜
- âœ… **ç‰ˆæœ¬å…¼å®¹**: å®Œæ•´å…¼å®¹å±‚
- âœ… **QK Norm**: æ­£ç¡®å®ç°
- âœ… **Router**: sigmoid + bias

</div>
<div>

### å®é™…æˆæœ
- âœ… **ç¼–è¯‘æˆåŠŸ**: 62å±‚å®Œæ•´ç¼–è¯‘
- âœ… **åŠ è½½æˆåŠŸ**: 230Bæƒé‡åˆ†ç‰‡
- âœ… **æ¨ç†è¿è¡Œ**: ç”Ÿæˆæµç¨‹å®Œæ•´
- âœ… **GPUä¸€è‡´æ€§**: æ‰€æœ‰å…³é”®ç»„ä»¶

</div>
</div>

---

## ç¼–è¯‘æ—¥å¿—éªŒè¯

```bash
# âœ… å…³é”®æˆåŠŸæ ‡å¿—
INFO:Neuron:Generating HLOs...

UserWarning: use_torch_block_wise set, using torch implementation
                    â†‘
              é…ç½®ç”Ÿæ•ˆï¼

INFO:Neuron:Generated all HLOs in 32.25 seconds
INFO:Neuron:Compilation completed successfully

# âœ… åŠ è½½æˆåŠŸ
INFO:Neuron:Done Sharding weights in 211.49 seconds
INFO:Neuron:Warmup completed in 2.39 seconds

Generating outputs... âœ…
```

---

## å…³é”®ä»£ç ä¿®æ”¹ç»Ÿè®¡

| ç±»åˆ« | æ–‡ä»¶æ•° | ä¿®æ”¹è¡Œæ•° | æ ¸å¿ƒä¿®æ”¹ |
|------|--------|---------|---------|
| **MoEåˆå§‹åŒ–** | 1 | ~20 | â­â­â­â­â­ |
| **QK Normå®ç°** | 1 | ~50 | â­â­â­â­â­ |
| **Router + Bias** | 1 | ~80 | â­â­â­â­â­ |
| **é…ç½®ä¿®å¤** | 1 | ~15 | â­â­â­â­ |
| **ç‰ˆæœ¬å…¼å®¹** | 2 | ~30 | â­â­â­â­ |
| **æ€»è®¡** | 5 | ~195 | - |

**ä»£ç æ”¹åŠ¨é‡å°ï¼Œä½†æ¯ä¸€å¤„éƒ½è‡³å…³é‡è¦**

---

<!-- _class: lead -->

# 7. ä¼˜åŒ–å»ºè®®ä¸å±•æœ›

---

## çŸ­æœŸä¼˜åŒ– (1-2å‘¨)

### âœ… å·²å®Œæˆçš„ä¼˜åŒ–

```python
# 1. QK Norm: æ­£ç¡®å®ç°åœ¨å®Œæ•´ç»´åº¦ä¸Šåº”ç”¨
self.q_norm = RMSNorm(num_heads * head_dim)
Q = self.q_norm(Q)  # åœ¨ reshape ä¹‹å‰

# 2. Router: sigmoid + e_score_correction_bias
router_config={'act_fn': 'sigmoid'}
RouterTopKWithBias(...)

# 3. FP8: scale å‚æ•°æ­£ç¡®è½¬æ¢
weight_scale_inv â†’ scale (47,864ä¸ª)
```

### ğŸ¯ ä¸‹ä¸€æ­¥: éªŒè¯è¾“å‡ºè´¨é‡

```bash
# è¿è¡Œæ ‡å‡† benchmark
python3 generation_minimax_m2_demo.py

# å¯¹æ¯” GPU è¾“å‡º
```

---

## å·²å®Œæˆçš„æ‰€æœ‰ä¿®å¤

| é—®é¢˜ | çŠ¶æ€ | å½±å“ |
|------|------|------|
| DGE ç¼–è¯‘é”™è¯¯ | âœ… å·²ä¿®å¤ | ä½¿ç”¨ torch blockwise |
| QK Norm | âœ… å·²ä¿®å¤ | ä¸ GPU ä¸€è‡´ |
| Router sigmoid | âœ… å·²ä¿®å¤ | router_config |
| e_score_correction_bias | âœ… å·²ä¿®å¤ | RouterTopKWithBias |
| FP8 scale | âœ… å·²ä¿®å¤ | 47,864 å‚æ•° |
| o_proj è·¯å¾„ | âœ… å·²ä¿®å¤ | preshard_hook |
| transformers å…¼å®¹ | âœ… å·²ä¿®å¤ | GenerationMixin |

---

## ä¸­æœŸä¼˜åŒ– (1-2æœˆ)

### ğŸ¯ å¯ç”¨NKI Kernel

**æŒ‘æˆ˜**: å¦‚ä½•ç»•è¿‡DGEé™åˆ¶ï¼Ÿ

1. **è°ƒæ•´intermediate_size** (éœ€é‡æ–°è®­ç»ƒ) âŒ
2. **é™ä½tp_degree** (ä¼šOOM) âŒ
3. **è”ç³»AWS Neuronå›¢é˜Ÿ** (é™ä½DGEè¦æ±‚) âœ…

**é¢„æœŸæ•ˆæœ**: æå‡20-40%æ¨ç†é€Ÿåº¦ â­â­â­

---

## ä¸­æœŸä¼˜åŒ– (ç»­)

### ğŸ¯ ä¼˜åŒ–MoEé…ç½®

```python
# å°è¯•çœŸæ­£å¯ç”¨Expert Parallelism
neuron_config = MoENeuronConfig(
    tp_degree=64,
    moe_ep_degree=8,   # 8ä¸ªexpertå¹¶è¡Œç»„
    moe_tp_degree=8,   # æ¯ç»„8-way TP
)
```

**éœ€è¦éªŒè¯**:
- é…ç½®æ˜¯å¦çœŸçš„ç”Ÿæ•ˆï¼ˆç›®å‰æœªç”Ÿæ•ˆï¼‰
- æ˜¯å¦æ”¹å–„è´Ÿè½½å‡è¡¡
- é€šä¿¡å¼€é”€ vs è®¡ç®—å¹¶è¡Œçš„æƒè¡¡

---

## é•¿æœŸå±•æœ›

### ğŸ”¬ æ··åˆç²¾åº¦ç­–ç•¥

```python
# ä¸åŒç»„ä»¶ç”¨ä¸åŒç²¾åº¦
neuron_config = MoENeuronConfig(
    attention_dtype=torch.bfloat16,    # é«˜ç²¾åº¦
    mlp_dtype=torch.float8_e4m3fn,    # èŠ‚çœå†…å­˜
    norm_dtype=torch.float32,          # å…³é”®æ“ä½œ
)
```

### ğŸ“ æ¨¡å‹è’¸é¦/å¾®è°ƒ

- åœ¨Neuronä¸Šç”¨shared normé‡æ–°å¾®è°ƒ
- è®©æ¨¡å‹é€‚åº”æ–°çš„normalizationæ–¹å¼
- éœ€è¦è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æº

---

## æŠ€æœ¯è·¯çº¿å›¾

```
å½“å‰ (2025-11)
â”œâ”€ âœ… ç¼–è¯‘æˆåŠŸ
â”œâ”€ âœ… åŠ è½½è¿è¡Œ
â””â”€ âš ï¸  è´¨é‡å¾…ä¼˜åŒ–

çŸ­æœŸ (1-2å‘¨)
â”œâ”€ ğŸ¯ Per-head QK Norm
â”œâ”€ âœ… FP8ç²¾åº¦ï¼ˆå·²å®Œæˆï¼‰
â””â”€ ğŸ“Š è´¨é‡è¯„ä¼°

ä¸­æœŸ (1-2æœˆ)
â”œâ”€ ğŸ¯ å¯ç”¨NKI Kernel
â”œâ”€ ğŸ¯ ä¼˜åŒ–MoEé…ç½®
â””â”€ ğŸ“Š æ€§èƒ½è¯„ä¼°

é•¿æœŸ (3æœˆ+)
â”œâ”€ ğŸ”¬ æ··åˆç²¾åº¦
â”œâ”€ ğŸ“ æ¨¡å‹å¾®è°ƒ
â””â”€ ğŸš€ ç”Ÿäº§éƒ¨ç½²
```

---

<!-- _class: lead -->

# æ€»ç»“ä¸å±•æœ›

---

## æ ¸å¿ƒæˆå°±

<div class="columns">
<div>

### æŠ€æœ¯çªç ´
1. âœ… è¯†åˆ«å¹¶è§£å†³moe_v2é…ç½®é—®é¢˜
2. âœ… æ­£ç¡®å®ç°QK Norm (ä¸GPUä¸€è‡´)
3. âœ… Router sigmoid + biasæ”¯æŒ
4. âœ… æ‰“é€šç«¯åˆ°ç«¯æ¨ç†æµç¨‹

</div>
<div>

### å·¥ç¨‹ä»·å€¼
1. ğŸ“– 230B MoEé€‚é…ç»éªŒ
2. ğŸ”§ å¯å¤ç”¨çš„é€‚é…æ¡†æ¶
3. ğŸ“Š è¯¦ç»†çš„GPU/Neuronå¯¹æ¯”
4. ğŸ¯ å®Œæ•´çš„å®ç°ä¸€è‡´æ€§

</div>
</div>

---

## å…³é”®ç»éªŒ

### âœ¨ æˆåŠŸç»éªŒ
- **é…ç½®è¿½è¸ª**: éªŒè¯é…ç½®æ˜¯å¦çœŸæ­£ç”Ÿæ•ˆ
- **æ¶æ„å¯¹æ¯”**: æ·±å…¥ç†è§£æ¨¡å‹å·®å¼‚
- **æ¸è¿›å¼è°ƒè¯•**: é€æ­¥å®šä½æ ¹æœ¬åŸå› 

### âš ï¸ æ•™è®­
- **ä¸è¦å‡è®¾**: Per-head vs Sharedçš„åŒºåˆ«
- **éªŒè¯é…ç½®**: moe vs moe_v2çš„é™·é˜±
- **ç‰ˆæœ¬ç®¡ç†**: transformerså¿«é€Ÿè¿­ä»£

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œ (æœ¬å‘¨)
1. âœ… æŠ€æœ¯æŠ¥å‘Šå·²å®Œæˆ
2. âœ… FP8 é‡åŒ–é—®é¢˜å·²è§£å†³
3. ğŸ”„ å®ç°per-head QK normæ”¯æŒ

### è¿‘æœŸè®¡åˆ’ (æœ¬æœˆ)
1. ğŸ”§ è”ç³»AWS Neuronå›¢é˜Ÿ
2. ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•
3. ğŸ“ æœ€ä½³å®è·µæ–‡æ¡£

---

<!-- _class: lead -->

# Q & A

## æ„Ÿè°¢è†å¬ï¼

**æŠ€æœ¯æŠ¥å‘Š**: `MiniMax_M2_Trainium2_Adaptation_Report.md`
**ä»£ç ä»“åº“**: `/home/ubuntu/neuronx-distributed-inference/`
**è”ç³»æ–¹å¼**: [æ‚¨çš„è”ç³»æ–¹å¼]

---

## é™„å½•: é—®é¢˜è¯Šæ–­æ¸…å•

### ç¼–è¯‘é˜¶æ®µ
- [ ] DGEé”™è¯¯ â†’ æ£€æŸ¥`use_torch_block_wise=True`
- [ ] Importé”™è¯¯ â†’ ç¡®è®¤ä½¿ç”¨`moe_v2`
- [ ] OOMé”™è¯¯ â†’ ç¡®è®¤`tp_degree=64`

### åŠ è½½é˜¶æ®µ
- [ ] QK normå½¢çŠ¶é”™è¯¯ â†’ æ£€æŸ¥reshapeé€»è¾‘
- [ ] Router dtypeé”™è¯¯ â†’ ç¡®è®¤`to_torch_dtype`è½¬æ¢
- [x] FP8é‡åŒ–é”™è¯¯ â†’ ä½¿ç”¨æ™ºèƒ½ç»•è¿‡æ–¹æ¡ˆï¼ˆå·²è§£å†³âœ…ï¼‰

### æ¨ç†é˜¶æ®µ
- [ ] GenerationMixiné”™è¯¯ â†’ ç¡®è®¤ç»§æ‰¿é¡ºåº
- [ ] è¾“å‡ºè´¨é‡å·® â†’ æ£€æŸ¥QK normæ˜¯å¦averaged

---

## é™„å½•: å¿«é€Ÿå‚è€ƒ

### å…³é”®é…ç½®
```python
neuron_config = MoENeuronConfig(
    tp_degree=64,
    blockwise_matmul_config={
        'use_torch_block_wise': True,  # æ ¸å¿ƒ
    }
)
```

### éªŒè¯å‘½ä»¤
```bash
# ç¼–è¯‘
python3 generation_minimax_m2_demo.py

# è·³è¿‡ç¼–è¯‘
python3 generation_minimax_m2_demo.py --skip-compile
```

---

<!-- _class: lead -->

# è°¢è°¢ï¼

**æŠ€æœ¯æ”¯æŒ**: AWS Neuron Team
**è´¡çŒ®è€…**: [æ‚¨çš„å›¢é˜Ÿ]
**æ—¥æœŸ**: 2025-11-05
