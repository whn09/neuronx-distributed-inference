====================================================================================================
MiniMax-M2 Checkpoint Precision Analysis
====================================================================================================

üìÅ Found 125 safetensor file(s):
  ‚Ä¢ model-00000-of-00130.safetensors
  ‚Ä¢ model-00001-of-00130.safetensors
  ‚Ä¢ model-00002-of-00130.safetensors
  ‚Ä¢ model-00003-of-00130.safetensors
  ‚Ä¢ model-00004-of-00130.safetensors
  ‚Ä¢ model-00005-of-00130.safetensors
  ‚Ä¢ model-00006-of-00130.safetensors
  ‚Ä¢ model-00007-of-00130.safetensors
  ‚Ä¢ model-00008-of-00130.safetensors
  ‚Ä¢ model-00009-of-00130.safetensors
  ‚Ä¢ model-00010-of-00130.safetensors
  ‚Ä¢ model-00011-of-00130.safetensors
  ‚Ä¢ model-00012-of-00130.safetensors
  ‚Ä¢ model-00013-of-00130.safetensors
  ‚Ä¢ model-00014-of-00130.safetensors
  ‚Ä¢ model-00015-of-00130.safetensors
  ‚Ä¢ model-00016-of-00130.safetensors
  ‚Ä¢ model-00017-of-00130.safetensors
  ‚Ä¢ model-00018-of-00130.safetensors
  ‚Ä¢ model-00019-of-00130.safetensors
  ‚Ä¢ model-00020-of-00130.safetensors
  ‚Ä¢ model-00021-of-00130.safetensors
  ‚Ä¢ model-00022-of-00130.safetensors
  ‚Ä¢ model-00023-of-00130.safetensors
  ‚Ä¢ model-00024-of-00130.safetensors
  ‚Ä¢ model-00025-of-00130.safetensors
  ‚Ä¢ model-00026-of-00130.safetensors
  ‚Ä¢ model-00027-of-00130.safetensors
  ‚Ä¢ model-00028-of-00130.safetensors
  ‚Ä¢ model-00029-of-00130.safetensors
  ‚Ä¢ model-00030-of-00130.safetensors
  ‚Ä¢ model-00031-of-00130.safetensors
  ‚Ä¢ model-00032-of-00130.safetensors
  ‚Ä¢ model-00033-of-00130.safetensors
  ‚Ä¢ model-00034-of-00130.safetensors
  ‚Ä¢ model-00035-of-00130.safetensors
  ‚Ä¢ model-00036-of-00130.safetensors
  ‚Ä¢ model-00037-of-00130.safetensors
  ‚Ä¢ model-00038-of-00130.safetensors
  ‚Ä¢ model-00039-of-00130.safetensors
  ‚Ä¢ model-00040-of-00130.safetensors
  ‚Ä¢ model-00041-of-00130.safetensors
  ‚Ä¢ model-00042-of-00130.safetensors
  ‚Ä¢ model-00043-of-00130.safetensors
  ‚Ä¢ model-00044-of-00130.safetensors
  ‚Ä¢ model-00045-of-00130.safetensors
  ‚Ä¢ model-00046-of-00130.safetensors
  ‚Ä¢ model-00047-of-00130.safetensors
  ‚Ä¢ model-00048-of-00130.safetensors
  ‚Ä¢ model-00049-of-00130.safetensors
  ‚Ä¢ model-00050-of-00130.safetensors
  ‚Ä¢ model-00051-of-00130.safetensors
  ‚Ä¢ model-00052-of-00130.safetensors
  ‚Ä¢ model-00053-of-00130.safetensors
  ‚Ä¢ model-00054-of-00130.safetensors
  ‚Ä¢ model-00055-of-00130.safetensors
  ‚Ä¢ model-00056-of-00130.safetensors
  ‚Ä¢ model-00057-of-00130.safetensors
  ‚Ä¢ model-00058-of-00130.safetensors
  ‚Ä¢ model-00059-of-00130.safetensors
  ‚Ä¢ model-00060-of-00130.safetensors
  ‚Ä¢ model-00061-of-00130.safetensors
  ‚Ä¢ model-00062-of-00130.safetensors
  ‚Ä¢ model-00063-of-00130.safetensors
  ‚Ä¢ model-00064-of-00130.safetensors
  ‚Ä¢ model-00065-of-00130.safetensors
  ‚Ä¢ model-00066-of-00130.safetensors
  ‚Ä¢ model-00067-of-00130.safetensors
  ‚Ä¢ model-00068-of-00130.safetensors
  ‚Ä¢ model-00069-of-00130.safetensors
  ‚Ä¢ model-00070-of-00130.safetensors
  ‚Ä¢ model-00071-of-00130.safetensors
  ‚Ä¢ model-00072-of-00130.safetensors
  ‚Ä¢ model-00073-of-00130.safetensors
  ‚Ä¢ model-00074-of-00130.safetensors
  ‚Ä¢ model-00075-of-00130.safetensors
  ‚Ä¢ model-00076-of-00130.safetensors
  ‚Ä¢ model-00077-of-00130.safetensors
  ‚Ä¢ model-00078-of-00130.safetensors
  ‚Ä¢ model-00079-of-00130.safetensors
  ‚Ä¢ model-00080-of-00130.safetensors
  ‚Ä¢ model-00081-of-00130.safetensors
  ‚Ä¢ model-00082-of-00130.safetensors
  ‚Ä¢ model-00083-of-00130.safetensors
  ‚Ä¢ model-00084-of-00130.safetensors
  ‚Ä¢ model-00085-of-00130.safetensors
  ‚Ä¢ model-00086-of-00130.safetensors
  ‚Ä¢ model-00087-of-00130.safetensors
  ‚Ä¢ model-00088-of-00130.safetensors
  ‚Ä¢ model-00089-of-00130.safetensors
  ‚Ä¢ model-00090-of-00130.safetensors
  ‚Ä¢ model-00091-of-00130.safetensors
  ‚Ä¢ model-00092-of-00130.safetensors
  ‚Ä¢ model-00093-of-00130.safetensors
  ‚Ä¢ model-00094-of-00130.safetensors
  ‚Ä¢ model-00095-of-00130.safetensors
  ‚Ä¢ model-00096-of-00130.safetensors
  ‚Ä¢ model-00097-of-00130.safetensors
  ‚Ä¢ model-00098-of-00130.safetensors
  ‚Ä¢ model-00099-of-00130.safetensors
  ‚Ä¢ model-00100-of-00130.safetensors
  ‚Ä¢ model-00101-of-00130.safetensors
  ‚Ä¢ model-00102-of-00130.safetensors
  ‚Ä¢ model-00103-of-00130.safetensors
  ‚Ä¢ model-00104-of-00130.safetensors
  ‚Ä¢ model-00105-of-00130.safetensors
  ‚Ä¢ model-00106-of-00130.safetensors
  ‚Ä¢ model-00107-of-00130.safetensors
  ‚Ä¢ model-00108-of-00130.safetensors
  ‚Ä¢ model-00109-of-00130.safetensors
  ‚Ä¢ model-00110-of-00130.safetensors
  ‚Ä¢ model-00111-of-00130.safetensors
  ‚Ä¢ model-00112-of-00130.safetensors
  ‚Ä¢ model-00113-of-00130.safetensors
  ‚Ä¢ model-00114-of-00130.safetensors
  ‚Ä¢ model-00115-of-00130.safetensors
  ‚Ä¢ model-00116-of-00130.safetensors
  ‚Ä¢ model-00117-of-00130.safetensors
  ‚Ä¢ model-00118-of-00130.safetensors
  ‚Ä¢ model-00119-of-00130.safetensors
  ‚Ä¢ model-00120-of-00130.safetensors
  ‚Ä¢ model-00121-of-00130.safetensors
  ‚Ä¢ model-00122-of-00130.safetensors
  ‚Ä¢ model-00123-of-00130.safetensors
  ‚Ä¢ model-00124-of-00130.safetensors

üîç Analyzing parameters...
‚úì Analyzed 96103 parameters

====================================================================================================
Data Type Distribution
====================================================================================================
  bfloat16            :    251 parameters (  0.3%)
  float32             :  47988 parameters ( 49.9%)
  float8_e4m3fn       :  47864 parameters ( 49.8%)

====================================================================================================
FP8 Quantization Status
====================================================================================================
‚úì Found 47864 FP8 scale parameters (weight_scale_inv)
  ‚Üí Model has FP8 quantized weights

  First 10 FP8 scale parameters:
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.0.w1.weight_scale_inv                    | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.0.w3.weight_scale_inv                    | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.1.w1.weight_scale_inv                    | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.1.w3.weight_scale_inv                    | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.10.w1.weight_scale_inv                   | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.10.w3.weight_scale_inv                   | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.100.w1.weight_scale_inv                  | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.100.w3.weight_scale_inv                  | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.101.w1.weight_scale_inv                  | dtype: float32    | shape: [12, 24]
    ‚Ä¢ model.layers.0.block_sparse_moe.experts.101.w3.weight_scale_inv                  | dtype: float32    | shape: [12, 24]
    ... and 47854 more

====================================================================================================
Layer-wise Precision Breakdown
====================================================================================================

üì¶ Embedding Layers (1 parameters):
  ‚Ä¢ model.embed_tokens.weight                                                        | bfloat16   | shape: [200064, 3072]

üîç Attention Layers (Layer 0 as example):
  ‚Ä¢ model.k_norm.weight                      | bfloat16   | shape: [1024]
  ‚Ä¢ model.k_proj.weight                      | float8_e4m3fn | shape: [1024, 3072]
  ‚Ä¢ model.k_proj.weight_scale_inv            | float32    | shape: [8, 24]
  ‚Ä¢ model.o_proj.weight                      | float8_e4m3fn | shape: [3072, 6144]
  ‚Ä¢ model.o_proj.weight_scale_inv            | float32    | shape: [24, 48]
  ‚Ä¢ model.q_norm.weight                      | bfloat16   | shape: [6144]
  ‚Ä¢ model.q_proj.weight                      | float8_e4m3fn | shape: [6144, 3072]
  ‚Ä¢ model.q_proj.weight_scale_inv            | float32    | shape: [48, 24]
  ‚Ä¢ model.v_proj.weight                      | float8_e4m3fn | shape: [1024, 3072]
  ‚Ä¢ model.v_proj.weight_scale_inv            | float32    | shape: [8, 24]
  ‚Üí Attention precision: ['bfloat16', 'float32', 'float8_e4m3fn']
  ‚Üí Has FP8 scales: Yes

üîÄ MoE Expert Layers (Layer 0, Expert 0 as example):
  ‚Ä¢ model.w1.weight                          | float8_e4m3fn | shape: [1536, 3072]
  ‚Ä¢ model.w1.weight_scale_inv                | float32    | shape: [12, 24]
  ‚Ä¢ model.w2.weight                          | float8_e4m3fn | shape: [3072, 1536]
  ‚Ä¢ model.w2.weight_scale_inv                | float32    | shape: [24, 12]
  ‚Ä¢ model.w3.weight                          | float8_e4m3fn | shape: [1536, 3072]
  ‚Ä¢ model.w3.weight_scale_inv                | float32    | shape: [12, 24]
  ‚Üí MoE precision: ['float8_e4m3fn', 'float32']
  ‚Üí Has FP8 scales: Yes

üéØ Router/Gate (Layer 0):
  ‚Ä¢ model.gate.weight                        | float32    | shape: [256, 3072]

üìê Normalization Layers (Layer 0 as example):
  ‚Ä¢ model.input_layernorm.weight             | bfloat16   | shape: [3072]

====================================================================================================
Summary
====================================================================================================

üìä Parameter Type Breakdown:
  ‚Ä¢ Embedding params: 1
  ‚Ä¢ Attention params: 682
  ‚Ä¢ MoE params: 95356
  ‚Ä¢ Normalization params: 63
  ‚Ä¢ Other params: 1
  ‚Ä¢ Total: 96103

üéØ Quantization Status:
  ‚úì Model IS FP8 quantized
  ‚Ä¢ Found 47864 FP8 scale parameters
  ‚Ä¢ Quantized modules: self_attn, moe_experts
  ‚Ä¢ NOT quantized: gate, lm_head

====================================================================================================
