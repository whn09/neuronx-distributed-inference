# FP8 Quantization: weight_scale_inv vs scale

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥

```
weight_scale_inv (HuggingFace) = 1 / scale (Neuron)
```

---

## ğŸ“Š å®Œæ•´æµç¨‹å›¾

```
è®­ç»ƒé˜¶æ®µ (Training)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  åŸå§‹æƒé‡ (bfloat16/float32)
  [6144, 3072] çŸ©é˜µ
  å€¼èŒƒå›´: ä¾‹å¦‚ [-15.2, +28.7]

         â”‚ FP8 é‡åŒ–
         â†“

  1. è®¡ç®—ç¼©æ”¾å› å­
     abs_max = 28.7
     fp8_max = 448.0
     scale = abs_max / fp8_max = 0.0641

  2. é‡åŒ–æƒé‡
     quantized_weight = original_weight / scale
     ç°åœ¨å€¼èŒƒå›´: [-237.4, +448.0]  âœ“ é€‚åˆFP8èŒƒå›´

  3. è½¬æ¢ä¸ºFP8æ ¼å¼
     weight_fp8 = quantized_weight.to(torch.float8_e4m3fn)

  4. ä¿å­˜æ—¶è®¡ç®—å€’æ•° (ä¸ºäº†æ¨ç†æ—¶å¿«é€Ÿ!)
     weight_scale_inv = 1 / scale = 15.625


å­˜å‚¨åœ¨Checkpoint
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“ model.safetensors

  layers.0.self_attn.q_proj.weight          â† FP8é‡åŒ–æƒé‡
    dtype: float8_e4m3fn
    shape: [6144, 3072]
    size: 6144 Ã— 3072 Ã— 1 byte = 18.4 MB

  layers.0.self_attn.q_proj.weight_scale_inv  â† Scaleçš„å€’æ•°
    dtype: float32
    shape: [48, 24]  (block-wise: æ¯128Ã—128ä¸€ä¸ªscale)
    size: 48 Ã— 24 Ã— 4 bytes = 4.6 KB


æ¨ç†é˜¶æ®µ (Inference)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

æ–¹æ³•1: ä½¿ç”¨HuggingFaceå‘½å (weight_scale_inv)

  dequantized = weight_fp8 * weight_scale_inv
                â†‘            â†‘
              é‡åŒ–æƒé‡      1/scale (å·²é¢„è®¡ç®—)

  = (original / scale) * (1 / scale)  âœ— é”™è¯¯!

  å®é™…ä¸ŠHFåœ¨æ¨ç†æ—¶:
  dequantized = weight_fp8 * weight_scale_inv
              = (original / scale) * scale  â† weight_scale_invåœ¨ä»–ä»¬é‚£è¾¹çš„è¯­ä¹‰ä¸åŒ!


æ–¹æ³•2: ä½¿ç”¨Neuronå‘½å (scale)

  Neuronæ¡†æ¶é¦–å…ˆè½¬æ¢:
    neuron_scale = 1.0 / hf_weight_scale_inv
                 = 1.0 / (1/scale)
                 = scale

  ç„¶åæ¨ç†:
    dequantized = weight_fp8 * neuron_scale
                = (original / scale) * scale
                = original  âœ“ æ­£ç¡®æ¢å¤!
```

---

## ğŸ”¢ æ•°å­¦å…³ç³»

```python
# è®­ç»ƒæ—¶ (Quantization)
scale = abs_max(weight) / fp8_max              # scaleç”¨äºé‡åŒ–
weight_fp8 = weight / scale                    # é‡åŒ–
weight_scale_inv = 1 / scale                   # ä¿å­˜å€’æ•°

# æ¨ç†æ—¶ (Dequantization)
# HFæ ¼å¼ â†’ Neuronæ ¼å¼è½¬æ¢
neuron_scale = 1 / weight_scale_inv            # è½¬å›scale
             = 1 / (1/scale)
             = scale

# æ¢å¤åŸå§‹æƒé‡
weight_original = weight_fp8 * neuron_scale
                = (weight/scale) * scale
                = weight
```

---

## ğŸ§± Block-wiseé‡åŒ–ç¤ºä¾‹

å¯¹äº `layers.0.self_attn.q_proj.weight`:

```
æƒé‡çŸ©é˜µ: [6144, 3072]
å—å¤§å°: [128, 128]

åˆ†å—æ–¹å¼:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ B00 â”‚ B01 â”‚ B02 â”‚...â”‚ B023â”‚  æ¯ä¸ªBxyæ˜¯128Ã—128çš„å—
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ B10 â”‚ B11 â”‚ B12 â”‚...â”‚ B123â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ B20 â”‚ B21 â”‚ B22 â”‚...â”‚ B223â”‚
â”‚ ... â”‚ ... â”‚ ... â”‚...â”‚ ... â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ B470â”‚ B471â”‚ B472â”‚...â”‚B4723â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
   â†‘                      â†‘
  24åˆ—å—                24åˆ—
  (3072/128)

48è¡Œå— (6144/128)

æ¯ä¸ªå—æœ‰ç‹¬ç«‹çš„scale:
scale_matrix shape: [48, 24]

scale[0,0] ç”¨äº Block B00
scale[0,1] ç”¨äº Block B01
scale[1,0] ç”¨äº Block B10
...

é‡åŒ–:
  B00_fp8 = B00 / scale[0,0]
  B01_fp8 = B01 / scale[0,1]
  ...

åé‡åŒ–:
  B00_original = B00_fp8 * scale[0,0]
  B01_original = B01_fp8 * scale[0,1]
  ...
```

---

## ğŸ’¾ å­˜å‚¨æ•ˆç‡å¯¹æ¯”

### åŸå§‹æ¨¡å‹ (bfloat16)
```
layers.0.self_attn.q_proj.weight: [6144, 3072]
å­˜å‚¨: 6144 Ã— 3072 Ã— 2 bytes = 36.9 MB
```

### FP8é‡åŒ–æ¨¡å‹
```
layers.0.self_attn.q_proj.weight: [6144, 3072] (float8_e4m3fn)
å­˜å‚¨: 6144 Ã— 3072 Ã— 1 byte = 18.4 MB

layers.0.self_attn.q_proj.weight_scale_inv: [48, 24] (float32)
å­˜å‚¨: 48 Ã— 24 Ã— 4 bytes = 4.6 KB

æ€»è®¡: 18.4 MB + 4.6 KB â‰ˆ 18.4 MB

èŠ‚çœ: (36.9 - 18.4) / 36.9 = 50.1% âœ“
```

æ•´ä¸ªæ¨¡å‹:
- åŸå§‹: 48,239ä¸ªbfloat16å‚æ•° â‰ˆ 96 GB
- FP8é‡åŒ–: 47,864ä¸ªFP8æƒé‡ + 47,864ä¸ªscale â‰ˆ 50 GB
- **èŠ‚çœçº¦ 46 GB å†…å­˜!**

---

## âš¡ æ¨ç†æ€§èƒ½æå‡

1. **å†…å­˜å¸¦å®½**: FP8æ˜¯bfloat16çš„ä¸€åŠå¤§å° â†’ åŠ è½½é€Ÿåº¦å¿«2å€
2. **è®¡ç®—é€Ÿåº¦**: ç¡¬ä»¶åŠ é€Ÿçš„FP8 GEMM (çŸ©é˜µä¹˜æ³•)æ¯”bfloat16å¿«
3. **KV Cache**: Attentionçš„K/Vç¼“å­˜ä¹Ÿä½¿ç”¨FP8 â†’ å¯æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡

---

## ğŸ” æ‚¨é‡åˆ°çš„é”™è¯¯åˆ†æ

### âŒ é”™è¯¯é…ç½®
```python
neuron_config = MoENeuronConfig(
    quantized_mlp_kernel_enabled=True,
    modules_to_not_convert=["lm_head", "self_attn"],  # â† é”™è¯¯!
)
```

ç³»ç»Ÿé€»è¾‘:
```
if "self_attn" in modules_to_not_convert:
    # ä¸æœŸæœ›attentionå±‚æœ‰scaleå‚æ•°
    expected_scale = None
else:
    # æœŸæœ›æœ‰scaleå‚æ•°
    expected_scale = load_scale_from_checkpoint()

# åŠ è½½æƒé‡
weight = load_weight()
scale = load_scale() if needs_scale else None

# éªŒè¯ç»´åº¦
if scale is not None:
    assert scale.shape[axis] == weight.shape[axis]  # â† åœ¨è¿™é‡Œå¤±è´¥!
```

é—®é¢˜:
1. Checkpointé‡Œ**æœ‰** `q_proj.weight_scale_inv` (å› ä¸ºattentionå±‚ç¡®å®è¢«FP8é‡åŒ–äº†)
2. ç³»ç»ŸåŠ è½½å¹¶è½¬æ¢ä¸º `scale`
3. ä½†å› ä¸º `"self_attn"` åœ¨ `modules_to_not_convert` ä¸­
4. æŸäº›ä»£ç è·¯å¾„æœŸæœ› `scale = None`
5. ç»´åº¦éªŒè¯æ—¶å‘ç° `scale` ä¸æ˜¯ `None` â†’ AssertionError!

### âœ… æ­£ç¡®é…ç½®
```python
neuron_config = MoENeuronConfig(
    quantized_mlp_kernel_enabled=True,
    modules_to_not_convert=["lm_head"],  # åªæ’é™¤çœŸæ­£æœªé‡åŒ–çš„å±‚
)
```

è¿™æ ·ç³»ç»ŸçŸ¥é“:
- Attentionå±‚**æœ‰**FP8é‡åŒ– â†’ æœŸæœ›å¹¶æ­£ç¡®å¤„ç† scale å‚æ•°
- MoE Expertå±‚**æœ‰**FP8é‡åŒ– â†’ æœŸæœ›å¹¶æ­£ç¡®å¤„ç† scale å‚æ•°
- lm_head **æ²¡æœ‰**FP8é‡åŒ– â†’ ä¸æœŸæœ› scale å‚æ•°

---

## ğŸ“š æ€»ç»“è¡¨

| æœ¯è¯­ | æ•°å­¦å«ä¹‰ | ç”¨é€” | ä½ç½® |
|------|----------|------|------|
| **scale** | `abs_max / fp8_max` | é‡åŒ–é™¤æ•° | è®­ç»ƒæ—¶è®¡ç®— |
| **weight_scale_inv** | `1 / scale` | åé‡åŒ–ä¹˜æ•° | HF checkpoint |
| **neuron_scale** | `1 / weight_scale_inv = scale` | åé‡åŒ–ä¹˜æ•° | Neuronæ¨ç† |

å…³é”®è½¬æ¢:
```python
# HF checkpoint â†’ Neuron framework
neuron_scale = 1.0 / hf_weight_scale_inv
```

æ¨ç†å…¬å¼:
```python
# æ¢å¤åŸå§‹æƒé‡
original_weight = quantized_weight_fp8 * neuron_scale
```

---

## ğŸ“ å»¶ä¼¸é˜…è¯»

FP8é‡åŒ–çš„ä¼˜åŠ¿:
1. å†…å­˜å ç”¨å‡åŠ
2. è®¡ç®—é€Ÿåº¦æå‡ (ç¡¬ä»¶åŠ é€Ÿ)
3. ç²¾åº¦æŸå¤±å¾ˆå° (ç‰¹åˆ«æ˜¯block-wiseé‡åŒ–)
4. é€‚åˆå¤§æ¨¡å‹æ¨ç†

ä¸ºä»€ä¹ˆä¸ç”¨INT8?
- FP8ä¿ç•™æµ®ç‚¹æ ¼å¼ï¼Œæ›´å®¹æ˜“å¤„ç†å¤§èŒƒå›´çš„å€¼
- FP8åœ¨Transformeræ¨¡å‹ä¸Šçš„ç²¾åº¦æŸå¤±æ¯”INT8å°
- æ–°ä¸€ä»£AIåŠ é€Ÿå™¨(å¦‚Neuron)å¯¹FP8æœ‰åŸç”Ÿæ”¯æŒ

MiniMax-M2çš„é‡åŒ–ç­–ç•¥:
- Attentionå±‚: FP8 âœ…
- MoE Expertå±‚: FP8 âœ…
- Router (gate): FP32 (éœ€è¦é«˜ç²¾åº¦åšexperté€‰æ‹©)
- Embedding/LM Head: bfloat16 (è¾“å…¥è¾“å‡ºä¿æŒé«˜ç²¾åº¦)
- LayerNorm: bfloat16 (å½’ä¸€åŒ–éœ€è¦é«˜ç²¾åº¦)
