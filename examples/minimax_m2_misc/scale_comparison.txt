╔════════════════════════════════════════════════════════════════════════════╗
║               weight_scale_inv  vs  scale 的区别                          ║
╚════════════════════════════════════════════════════════════════════════════╝

┌──────────────────────────────────────────────────────────────────────────┐
│ 1️⃣  训练时的量化过程                                                     │
└──────────────────────────────────────────────────────────────────────────┘

   原始权重
   ┌─────────────────┐
   │ 10.5, -8.2, ... │  bfloat16, 范围: [-15.0, +20.0]
   └─────────────────┘
           │
           │ (1) 计算scale
           │     scale = abs_max / 448 = 20.0 / 448 = 0.04464
           ↓
   ┌─────────────────┐
   │ 量化: w / scale │
   └─────────────────┘
           │
           ↓
   ┌─────────────────┐
   │ 235, -184, ...  │  FP8, 范围现在在 [-336, +448]
   └─────────────────┘
           │
           │ (2) 保存到checkpoint
           ↓

┌──────────────────────────────────────────────────────────────────────────┐
│ 💾 HuggingFace Checkpoint 存储格式                                        │
└──────────────────────────────────────────────────────────────────────────┘

  ✓ weight (FP8):              [量化后的值]
  ✓ weight_scale_inv (FP32):   1/0.04464 = 22.4

  为什么存 1/scale？
  → 因为乘法比除法快!
  → 恢复时: weight_fp8 × weight_scale_inv (快)
  → 而不是: weight_fp8 ÷ scale (慢)


┌──────────────────────────────────────────────────────────────────────────┐
│ 2️⃣  Neuron加载时的转换                                                   │
└──────────────────────────────────────────────────────────────────────────┘

  HF checkpoint           转换             Neuron state_dict
  ─────────────────────────────────────────────────────────────
  weight (FP8)      ──────────→           weight (FP8)
    235, -184, ...   [直接复制]            235, -184, ...

  weight_scale_inv  ──────────→           scale
    22.4              [取倒数]              0.04464
                     1 / 22.4 = 0.04464


┌──────────────────────────────────────────────────────────────────────────┐
│ 3️⃣  推理时的反量化                                                       │
└──────────────────────────────────────────────────────────────────────────┘

  Neuron推理引擎:

     weight_fp8  ×  scale  =  original_weight
        │            │              │
        │            │              └── 10.5, -8.2, ... (恢复!)
        │            └── 0.04464 (从转换后的scale)
        └── 235, -184, ... (FP8量化值)

  数学验证:
     235 × 0.04464 ≈ 10.5  ✓
    -184 × 0.04464 ≈ -8.2  ✓


╔════════════════════════════════════════════════════════════════════════════╗
║                            关键公式                                        ║
╚════════════════════════════════════════════════════════════════════════════╝

  HF命名:  weight_scale_inv = 1 / scale

  Neuron转换:  neuron_scale = 1 / weight_scale_inv
                             = 1 / (1/scale)
                             = scale

  推理:  original = weight_fp8 × neuron_scale


╔════════════════════════════════════════════════════════════════════════════╗
║                      您的模型中的实际例子                                  ║
╚════════════════════════════════════════════════════════════════════════════╝

layers.0.self_attn.q_proj.weight:
  ┌──────────────────────────────────────────────────────┐
  │ HF Checkpoint:                                       │
  │   weight: float8_e4m3fn, [6144, 3072]                │
  │   weight_scale_inv: float32, [48, 24]                │
  │                                                       │
  │ 转换后 (Neuron):                                     │
  │   qkv_proj.q_proj.weight: float8_e4m3fn, [6144,3072]│
  │   qkv_proj.q_proj.scale: float32, [48, 24]          │
  │                           ↑                           │
  │                  每个128×128的block一个scale         │
  └──────────────────────────────────────────────────────┘

Block-wise量化:
  weight[0:128, 0:128]   用  scale[0, 0]   反量化
  weight[0:128, 128:256] 用  scale[0, 1]   反量化
  weight[128:256, 0:128] 用  scale[1, 0]   反量化
  ...

这样每个block都有最优的精度!


╔════════════════════════════════════════════════════════════════════════════╗
║                        为什么您遇到错误                                    ║
╚════════════════════════════════════════════════════════════════════════════╝

modules_to_not_convert=["lm_head", "self_attn"]  ← 错误配置
                                    ^^^^^^^^^
                                    这告诉系统: "attention层没有量化"

但实际上:
  ✓ Checkpoint里有 q_proj.weight_scale_inv
  ✓ 系统加载并转换为 q_proj.scale
  ✓ 维度是 [48, 24]

系统检查:
  if "self_attn" not in modules_to_not_convert:
      assert scale.shape[axis] == weight.shape[axis]  ← 应该执行这个
  else:
      # 不检查scale，因为不应该有
      ...

但由于配置错误，有些代码路径混淆了，导致:
  assert scale.shape[0] == weight.shape[0]
          [48]               [6144]         ← 维度不匹配! 因为是block-wise

实际上应该是:
  48 = 6144 / 128  (block数量)
  24 = 3072 / 128


正确配置:
  modules_to_not_convert=["lm_head"]  ← 只排除真的没量化的
