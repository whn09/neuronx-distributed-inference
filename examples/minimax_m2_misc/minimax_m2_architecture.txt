The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.
The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.

====================================================================================================
  MiniMax-M2 Architecture Visualization
====================================================================================================

ğŸ“Š Model Configuration:
  â€¢ Architecture: MiniMaxM2ForCausalLM
  â€¢ Model Type: minimax_m2
  â€¢ Total Layers: 62
  â€¢ Hidden Dimension: 3072
  â€¢ Vocabulary Size: 200,064

====================================================================================================
  Layer-by-Layer Structure
====================================================================================================

MiniMaxM2ForCausalLM
â”‚
â”œâ”€â”€ model (MiniMaxM2Model)
â”‚   â”‚
â”‚   â”œâ”€â”€ embed_tokens (Embedding)
â”‚   â”‚   â””â”€â”€ [vocab_size=200064, hidden_size=3072]
â”‚   â”‚
â”‚   â”œâ”€â”€ layers (ModuleList[62])
â”‚   â”‚   â”‚

â”‚   â”‚   â”œâ”€â”€ Layer[0...61] (MiniMaxM2DecoderLayer)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm (RMSNorm)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ normalized_shape: [3072], eps=1e-06
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (MiniMaxM2Attention - Group Query Attention)
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj (Linear): [3072 â†’ 3072]
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Output: 48 attention heads
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Head dim: 64
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ q_norm (RMSNorm): per-head normalization
â”‚   â”‚   â”‚   â”‚   â”‚       â€¢ Shape: [48, 64]
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj (Linear): [3072 â†’ 512]
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Output: 8 KV heads (GQA)
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Head dim: 64
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Ratio: 6:1 (Q:KV)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ k_norm (RMSNorm): per-head normalization
â”‚   â”‚   â”‚   â”‚   â”‚       â€¢ Shape: [8, 64]
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj (Linear): [3072 â†’ 512]
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Output: 8 KV heads
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ o_proj (Linear): [3072 â†’ 3072]
â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Merges all attention heads back
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rotary_emb (RotaryEmbedding)
â”‚   â”‚   â”‚   â”‚       â€¢ Theta: 5,000,000
â”‚   â”‚   â”‚   â”‚       â€¢ Max positions: 196,608
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ post_attention_layernorm (RMSNorm)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ normalized_shape: [3072], eps=1e-06
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ block_sparse_moe (MoE - Mixture of Experts)
â”‚   â”‚   â”‚       â”‚
â”‚   â”‚   â”‚       â”œâ”€â”€ gate (Linear): [3072 â†’ 256]
â”‚   â”‚   â”‚       â”‚   â€¢ Routes input to top-8 experts
â”‚   â”‚   â”‚       â”‚
â”‚   â”‚   â”‚       â”œâ”€â”€ experts (ModuleList[256])
â”‚   â”‚   â”‚       â”‚   â”‚   Each expert is an FFN:
â”‚   â”‚   â”‚       â”‚   â”‚
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Expert[0...255] (MiniMaxM2MLP)
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ w1 (gate_proj): [3072 â†’ 1536]
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â€¢ For SwiGLU activation (gate path)
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ w3 (up_proj): [3072 â†’ 1536]
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â€¢ For SwiGLU activation (up path)
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ activation: SwiGLU
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â€¢ Combines: silu(w1(x)) * w3(x)
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ w2 (down_proj): [1536 â†’ 3072]
â”‚   â”‚   â”‚       â”‚   â”‚       â€¢ Projects back to hidden dimension
â”‚   â”‚   â”‚       â”‚   â”‚
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ ... (repeated for all experts)
â”‚   â”‚   â”‚       â”‚
â”‚   â”‚   â”‚       â””â”€â”€ (no e_score_correction_bias)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ... (repeated for 62 layers)
â”‚   â”‚
â”‚   â””â”€â”€ norm (RMSNorm - Final)
â”‚       â””â”€â”€ normalized_shape: [3072], eps=1e-06
â”‚
â””â”€â”€ lm_head (Linear)
    â””â”€â”€ [3072 â†’ 200,064]
        â€¢ Generates vocabulary logits for next token prediction

====================================================================================================
  Parameter Count Breakdown
====================================================================================================

ğŸ“Š Embedding Layer:
  â€¢ Parameters: 614,596,608

ğŸ“Š Per Decoder Layer (62 total):
  â€¢ Attention: 22,023,680 params
    - Q/K/V/O projections: 22,023,680 params
    - Q/K norms: 3,584 params
  â€¢ MoE MLP: 3,624,665,088 params
    - Router (gate): 786,432 params
    - 256 Experts: 3,623,878,656 params
    - Per expert: 14,155,776 params
  â€¢ LayerNorms: 6,144 params
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ Total per layer: 3,646,694,912 params

ğŸ“Š All 62 Layers:
  â€¢ Total: 226,095,084,544 params

ğŸ“Š Final LayerNorm:
  â€¢ Parameters: 3,072

ğŸ“Š LM Head (usually tied with embedding):
  â€¢ Parameters: 614,596,608 (if not tied)

====================================================================================================
  ğŸ¯ TOTAL MODEL PARAMETERS: 226,709,684,224
====================================================================================================

ğŸ“ˆ Model Characteristics:
  â€¢ Sparsity: 8/256 experts activated
  â€¢ Sparsity ratio: 3.1%
  â€¢ Effective MoE params per token: 114,032,640
  â€¢ Attention type: Group Query Attention (GQA)
  â€¢ GQA ratio: 6:1 (Query:KV)
  â€¢ Use QK normalization: True

====================================================================================================
  Key Features
====================================================================================================

âœ¨ Architecture Highlights:

1. ğŸ¯ Group Query Attention (GQA)
   â€¢ Reduces KV cache size while maintaining model quality
   â€¢ Q heads: 48, KV heads: 8 (6:1 ratio)
   â€¢ Per-head Q/K normalization for training stability

2. ğŸ”€ Mixture of Experts (MoE)
   â€¢ 256 experts per layer (massive expert pool)
   â€¢ Top-8 expert routing (sparse activation)
   â€¢ Only ~3.125% of experts active per token
   â€¢ Enables scaling model capacity without proportional compute increase

3. ğŸ¨ SwiGLU Activation
   â€¢ silu(w1(x)) âŠ— w3(x) â†’ w2(x)
   â€¢ Gated activation for better expressiveness
   â€¢ Separate gate and up projections

4. ğŸ”„ RoPE (Rotary Position Embedding)
   â€¢ Theta: 5,000,000 (supports very long contexts)
   â€¢ Max positions: 196,608 tokens (~200K context length!)
   â€¢ Relative positional encoding

5. ğŸ“ RMSNorm
   â€¢ Lightweight normalization (no bias, no mean centering)
   â€¢ Lower computational cost than LayerNorm
   â€¢ eps=1e-6 for numerical stability

====================================================================================================
Visualization complete!
====================================================================================================
