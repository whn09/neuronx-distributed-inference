# MiniMax-M2 FP8 é‡åŒ–é—®é¢˜å®Œæ•´è§£æ

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**é—®é¢˜**: è¿è¡Œ `python generation_minimax_m2_demo.py` æ—¶é‡åˆ° `AssertionError`

**åŸå› **: `modules_to_not_convert` é…ç½®é”™è¯¯ï¼ŒåŒ…å«äº† `"self_attn"`

## ğŸ“Š å…³é”®å‘ç°

### 1. æ¨¡å‹ç¡®å®ä½¿ç”¨FP8é‡åŒ–

é€šè¿‡ `check_model_precision.py` åˆ†æcheckpointå‘ç°ï¼š

```
âœ… é‡åŒ–çš„æ¨¡å— (æœ‰FP8æƒé‡+scales):
  - Attentionå±‚ (q/k/v/o_proj): float8_e4m3fn + weight_scale_inv
  - MoE Expertå±‚ (w1/w2/w3): float8_e4m3fn + weight_scale_inv
  - æ€»è®¡: 47,864 ä¸ªFP8 scaleå‚æ•°

âŒ æœªé‡åŒ–çš„æ¨¡å—:
  - Router (gate): float32
  - LM Head: bfloat16
  - Embedding: bfloat16
  - LayerNorm: bfloat16
```

### 2. weight_scale_inv vs scale

| æœ¯è¯­ | å«ä¹‰ | å…¬å¼ | ç”¨é€” |
|------|------|------|------|
| `weight_scale_inv` | Scaleçš„å€’æ•° | `1/scale` | HuggingFaceå­˜å‚¨æ ¼å¼ |
| `scale` | åé‡åŒ–å› å­ | `1/weight_scale_inv` | Neuronæ¨ç†æ¡†æ¶ |

**è½¬æ¢ä»£ç ** (modeling_minimax_m2.py:124-139):
```python
if config.neuron_config.quantized_mlp_kernel_enabled:
    for param_name in param_name_list:
        if param_name.endswith(".weight_scale_inv"):
            new_param_name = param_name.replace(".weight_scale_inv", ".scale")
            scale_inv = neuron_state_dict[param_name]
            neuron_state_dict[new_param_name] = 1.0 / scale_inv  # å–å€’æ•°!
            del neuron_state_dict[param_name]
```

### 3. Block-wiseé‡åŒ–

MiniMax-M2ä½¿ç”¨ block_size=[128, 128] çš„åˆ†å—é‡åŒ–:

```
weight shape: [6144, 3072]
block size:   [128, 128]
scale shape:  [48, 24]  â† 6144/128 Ã— 3072/128
```

æ¯ä¸ª128Ã—128çš„å—æœ‰ç‹¬ç«‹çš„scaleï¼Œæä¾›æ›´å¥½çš„ç²¾åº¦ã€‚

## âœ… è§£å†³æ–¹æ¡ˆ

### é”™è¯¯é…ç½® âŒ
```python
modules_to_not_convert=[
    "lm_head",
    "self_attn",  # â† é”™è¯¯! Attentionå±‚æ˜¯FP8é‡åŒ–çš„!
]
```

### æ­£ç¡®é…ç½® âœ…
```python
modules_to_not_convert=[
    "lm_head",  # åªæ’é™¤çœŸæ­£æœªé‡åŒ–çš„æ¨¡å—
    # gateå’Œe_score_correction_biaså·²ç»åœ¨æ¶æ„ä¸­æ’é™¤
]
```

## ğŸ” get_state_dict å·¥ä½œæµç¨‹

1. **åŠ è½½safetensors** (line 576-578)
   ```python
   with safe_open(shard_path, framework="pt", device="cpu") as f:
       for key in f.keys():
           model_sd[key] = f.get_tensor(key)
   ```

2. **ç§»é™¤"model."å‰ç¼€** (line 601-604)
   ```python
   if param_name.startswith("model."):
       updated_param_name = param_name.replace("model.", "", 1)
   ```

3. **è½¬æ¢FP8 scales** (line 124-139 in convert_hf_to_neuron)
   ```python
   if param_name.endswith(".weight_scale_inv"):
       new_param_name = param_name.replace(".weight_scale_inv", ".scale")
       neuron_state_dict[new_param_name] = 1.0 / neuron_state_dict[param_name]
   ```

4. **é‡å‘½åattentionå‚æ•°** (line 147-177)
   ```python
   # q_proj â†’ qkv_proj.q_proj
   # k_proj â†’ qkv_proj.k_proj
   # v_proj â†’ qkv_proj.v_proj
   # o_proj â†’ o_proj.o_proj
   ```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŠ¿

### å†…å­˜èŠ‚çœ
```
åŸå§‹ (bfloat16):  96,103 å‚æ•° Ã— 2 bytes = ~192 MB (per layer)
FP8é‡åŒ–:          47,864 weights Ã— 1 byte + 47,864 scales Ã— 4 bytes
                = ~48 MB + ~192 KB â‰ˆ 48 MB (per layer)

èŠ‚çœ: ~75% å†…å­˜
```

æ•´ä¸ªæ¨¡å‹: **èŠ‚çœçº¦ 46 GB**

### æ¨ç†é€Ÿåº¦
- å†…å­˜å¸¦å®½éœ€æ±‚å‡åŠ
- ç¡¬ä»¶åŠ é€Ÿçš„FP8 GEMM
- æ›´å¤§çš„batch sizeæˆ–æ›´é•¿çš„context

## ğŸ› ï¸ è°ƒè¯•å·¥å…·

å·²åˆ›å»ºçš„è„šæœ¬:

1. **check_model_precision.py**
   - åˆ†æcheckpointä¸­æ¯å±‚çš„ç²¾åº¦
   - è¯†åˆ«FP8é‡åŒ–çš„æ¨¡å—
   - è¾“å‡º: `model_precision_report.txt`

2. **debug_get_state_dict.py**
   - è°ƒè¯•state_dictåŠ è½½æµç¨‹
   - éªŒè¯FP8 scaleè½¬æ¢
   - æ£€æŸ¥å‚æ•°é‡å‘½å

3. **explain_fp8_scales.py**
   - è¯¦ç»†è§£é‡ŠFP8é‡åŒ–åŸç†
   - æ¼”ç¤ºé‡åŒ–/åé‡åŒ–è¿‡ç¨‹

4. **visualize_minimax_structure.py**
   - å¯è§†åŒ–æ¨¡å‹æ¶æ„
   - è¾“å‡º: `minimax_m2_architecture.txt`

## ğŸ“ ä¸‹ä¸€æ­¥

1. âœ… å·²ä¿®æ­£ `generation_minimax_m2_demo.py` é…ç½®
2. â­ï¸ é‡æ–°ç¼–è¯‘æ¨¡å‹:
   ```bash
   rm -rf /home/ubuntu/traced_model/MiniMax-M2/
   python generation_minimax_m2_demo.py
   ```
3. â­ï¸ éªŒè¯ç¼–è¯‘è¾“å‡º:
   ```
   === Converting FP8 scale parameters ===
     Total converted: 47864 FP8 scale parameters  â† åº”è¯¥æ˜¯47864!
   ```

## ğŸ“ å…³é”®å­¦ä¹ ç‚¹

1. **FP8é‡åŒ–ä¸ä¼šè‡ªåŠ¨åº”ç”¨** - å¿…é¡»æœ‰checkpointæ”¯æŒ
2. **Scaleå‚æ•°æ˜¯å¿…éœ€çš„** - ç”¨äºåé‡åŒ–å›é«˜ç²¾åº¦
3. **Block-wiseé‡åŒ–æ›´ç²¾ç¡®** - æ¯ä¸ªå—ç‹¬ç«‹scale
4. **é…ç½®å¿…é¡»åŒ¹é…checkpoint** - modules_to_not_convertè¦å‡†ç¡®
5. **ä¸¤å¥—å‘½åçº¦å®š** - HuggingFace vs Neuronéœ€è¦è½¬æ¢

## ğŸ“š å‚è€ƒæ–‡ä»¶

- ç²¾åº¦æŠ¥å‘Š: `model_precision_report.txt`
- æ¶æ„å›¾: `minimax_m2_architecture.txt`
- å¯è§†åŒ–æŒ‡å—: `fp8_visual_guide.md`
- å¯¹æ¯”å›¾: `scale_comparison.txt`

---

**ç»“è®º**: `get_state_dict` å®ç°æ˜¯æ­£ç¡®çš„ï¼Œé—®é¢˜å‡ºåœ¨é…ç½®ä¸Šã€‚ç§»é™¤ `"self_attn"` åä¸€åˆ‡æ­£å¸¸ï¼
