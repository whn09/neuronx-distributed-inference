#!/usr/bin/env python3
"""
Visualize MiniMax-M2 Model Architecture
"""

from transformers import AutoConfig
import json

model_path = "/home/ubuntu/model_hf/MiniMax-M2/"

# Load config
config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)

def print_section(title):
    print("\n" + "=" * 100)
    print(f"  {title}")
    print("=" * 100)

def print_subsection(title):
    print(f"\n{'â”€' * 100}")
    print(f"  {title}")
    print('â”€' * 100)

print_section("MiniMax-M2 Architecture Visualization")

print("\nğŸ“Š Model Configuration:")
print(f"  â€¢ Architecture: {config.architectures[0]}")
print(f"  â€¢ Model Type: {config.model_type}")
print(f"  â€¢ Total Layers: {config.num_hidden_layers}")
print(f"  â€¢ Hidden Dimension: {config.hidden_size}")
print(f"  â€¢ Vocabulary Size: {config.vocab_size:,}")

print_section("Layer-by-Layer Structure")

print("""
MiniMaxM2ForCausalLM
â”‚
â”œâ”€â”€ model (MiniMaxM2Model)
â”‚   â”‚
â”‚   â”œâ”€â”€ embed_tokens (Embedding)
â”‚   â”‚   â””â”€â”€ [vocab_size={}, hidden_size={}]
â”‚   â”‚
â”‚   â”œâ”€â”€ layers (ModuleList[{}])
â”‚   â”‚   â”‚
""".format(config.vocab_size, config.hidden_size, config.num_hidden_layers))

# Detailed layer structure
print(f"â”‚   â”‚   â”œâ”€â”€ Layer[0...{config.num_hidden_layers-1}] (MiniMaxM2DecoderLayer)")
print( "â”‚   â”‚   â”‚   â”‚")
print( "â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm (RMSNorm)")
print(f"â”‚   â”‚   â”‚   â”‚   â””â”€â”€ normalized_shape: [{config.hidden_size}], eps={config.rms_norm_eps}")
print( "â”‚   â”‚   â”‚   â”‚")
print( "â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (MiniMaxM2Attention - Group Query Attention)")
print( "â”‚   â”‚   â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj (Linear): [{config.hidden_size} â†’ {config.num_attention_heads * (config.hidden_size // config.num_attention_heads)}]")
print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Output: {config.num_attention_heads} attention heads")
print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Head dim: {config.hidden_size // config.num_attention_heads}")

if config.use_qk_norm:
    print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ q_norm (RMSNorm): per-head normalization")
    print(f"â”‚   â”‚   â”‚   â”‚   â”‚       â€¢ Shape: [{config.num_attention_heads}, {config.hidden_size // config.num_attention_heads}]")

print(f"â”‚   â”‚   â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj (Linear): [{config.hidden_size} â†’ {config.num_key_value_heads * (config.hidden_size // config.num_attention_heads)}]")
print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Output: {config.num_key_value_heads} KV heads (GQA)")
print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Head dim: {config.hidden_size // config.num_attention_heads}")
print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Ratio: {config.num_attention_heads // config.num_key_value_heads}:1 (Q:KV)")

if config.use_qk_norm:
    print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ k_norm (RMSNorm): per-head normalization")
    print(f"â”‚   â”‚   â”‚   â”‚   â”‚       â€¢ Shape: [{config.num_key_value_heads}, {config.hidden_size // config.num_attention_heads}]")

print(f"â”‚   â”‚   â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj (Linear): [{config.hidden_size} â†’ {config.num_key_value_heads * (config.hidden_size // config.num_attention_heads)}]")
print(f"â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Output: {config.num_key_value_heads} KV heads")
print( "â”‚   â”‚   â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ o_proj (Linear): [{config.num_attention_heads * (config.hidden_size // config.num_attention_heads)} â†’ {config.hidden_size}]")
print( "â”‚   â”‚   â”‚   â”‚   â”‚   â€¢ Merges all attention heads back")
print( "â”‚   â”‚   â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rotary_emb (RotaryEmbedding)")
print(f"â”‚   â”‚   â”‚   â”‚       â€¢ Theta: {config.rope_theta:,}")
print(f"â”‚   â”‚   â”‚   â”‚       â€¢ Max positions: {config.max_position_embeddings:,}")
print( "â”‚   â”‚   â”‚   â”‚")
print( "â”‚   â”‚   â”‚   â”œâ”€â”€ post_attention_layernorm (RMSNorm)")
print(f"â”‚   â”‚   â”‚   â”‚   â””â”€â”€ normalized_shape: [{config.hidden_size}], eps={config.rms_norm_eps}")
print( "â”‚   â”‚   â”‚   â”‚")
print( "â”‚   â”‚   â”‚   â””â”€â”€ block_sparse_moe (MoE - Mixture of Experts)")
print( "â”‚   â”‚   â”‚       â”‚")
print(f"â”‚   â”‚   â”‚       â”œâ”€â”€ gate (Linear): [{config.hidden_size} â†’ {config.num_local_experts}]")
print(f"â”‚   â”‚   â”‚       â”‚   â€¢ Routes input to top-{config.num_experts_per_tok} experts")
print( "â”‚   â”‚   â”‚       â”‚")
print(f"â”‚   â”‚   â”‚       â”œâ”€â”€ experts (ModuleList[{config.num_local_experts}])")
print(f"â”‚   â”‚   â”‚       â”‚   â”‚   Each expert is an FFN:")
print( "â”‚   â”‚   â”‚       â”‚   â”‚")
print(f"â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Expert[0...{config.num_local_experts-1}] (MiniMaxM2MLP)")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ w1 (gate_proj): [{config.hidden_size} â†’ {config.intermediate_size}]")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â€¢ For SwiGLU activation (gate path)")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ w3 (up_proj): [{config.hidden_size} â†’ {config.intermediate_size}]")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â€¢ For SwiGLU activation (up path)")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ activation: SwiGLU")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â€¢ Combines: silu(w1(x)) * w3(x)")
print( "â”‚   â”‚   â”‚       â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ w2 (down_proj): [{config.intermediate_size} â†’ {config.hidden_size}]")
print( "â”‚   â”‚   â”‚       â”‚   â”‚       â€¢ Projects back to hidden dimension")
print( "â”‚   â”‚   â”‚       â”‚   â”‚")
print( "â”‚   â”‚   â”‚       â”‚   â””â”€â”€ ... (repeated for all experts)")
print( "â”‚   â”‚   â”‚       â”‚")
if hasattr(config, 'e_score_correction_bias') and config.e_score_correction_bias is not None:
    print( "â”‚   â”‚   â”‚       â””â”€â”€ e_score_correction_bias (Parameter)")
    print(f"â”‚   â”‚   â”‚           â€¢ Shape: [{config.num_local_experts}]")
    print( "â”‚   â”‚   â”‚           â€¢ Adjusts expert selection scores")
else:
    print( "â”‚   â”‚   â”‚       â””â”€â”€ (no e_score_correction_bias)")
print( "â”‚   â”‚   â”‚")
print(f"â”‚   â”‚   â””â”€â”€ ... (repeated for {config.num_hidden_layers} layers)")
print( "â”‚   â”‚")
print( "â”‚   â””â”€â”€ norm (RMSNorm - Final)")
print(f"â”‚       â””â”€â”€ normalized_shape: [{config.hidden_size}], eps={config.rms_norm_eps}")
print( "â”‚")
print( "â””â”€â”€ lm_head (Linear)")
print(f"    â””â”€â”€ [{config.hidden_size} â†’ {config.vocab_size:,}]")
print( "        â€¢ Generates vocabulary logits for next token prediction")

print_section("Parameter Count Breakdown")

# Calculate parameters
embed_params = config.vocab_size * config.hidden_size
lm_head_params = config.hidden_size * config.vocab_size  # Usually tied with embedding

# Per layer
attn_params_per_layer = (
    config.hidden_size * config.num_attention_heads * (config.hidden_size // config.num_attention_heads) +  # q_proj
    config.hidden_size * config.num_key_value_heads * (config.hidden_size // config.num_attention_heads) * 2 +  # k_proj + v_proj
    config.num_attention_heads * (config.hidden_size // config.num_attention_heads) * config.hidden_size  # o_proj
)

if config.use_qk_norm:
    qk_norm_params = config.num_attention_heads * (config.hidden_size // config.num_attention_heads) + \
                     config.num_key_value_heads * (config.hidden_size // config.num_attention_heads)
    attn_params_per_layer += qk_norm_params

# MoE params per layer
expert_params = config.num_local_experts * (
    config.hidden_size * config.intermediate_size +  # w1
    config.hidden_size * config.intermediate_size +  # w3
    config.intermediate_size * config.hidden_size    # w2
)
gate_params = config.hidden_size * config.num_local_experts

moe_params_per_layer = expert_params + gate_params

# LayerNorm params
layernorm_params_per_layer = config.hidden_size * 2  # input_layernorm + post_attention_layernorm
final_layernorm_params = config.hidden_size

# Total
total_layer_params = config.num_hidden_layers * (
    attn_params_per_layer + moe_params_per_layer + layernorm_params_per_layer
)

total_params = embed_params + total_layer_params + final_layernorm_params  # lm_head usually tied

print(f"\nğŸ“Š Embedding Layer:")
print(f"  â€¢ Parameters: {embed_params:,}")

print(f"\nğŸ“Š Per Decoder Layer ({config.num_hidden_layers} total):")
print(f"  â€¢ Attention: {attn_params_per_layer:,} params")
print(f"    - Q/K/V/O projections: {attn_params_per_layer:,} params")
if config.use_qk_norm:
    print(f"    - Q/K norms: {qk_norm_params:,} params")
print(f"  â€¢ MoE MLP: {moe_params_per_layer:,} params")
print(f"    - Router (gate): {gate_params:,} params")
print(f"    - {config.num_local_experts} Experts: {expert_params:,} params")
print(f"    - Per expert: {expert_params // config.num_local_experts:,} params")
print(f"  â€¢ LayerNorms: {layernorm_params_per_layer:,} params")
print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  â€¢ Total per layer: {attn_params_per_layer + moe_params_per_layer + layernorm_params_per_layer:,} params")

print(f"\nğŸ“Š All {config.num_hidden_layers} Layers:")
print(f"  â€¢ Total: {total_layer_params:,} params")

print(f"\nğŸ“Š Final LayerNorm:")
print(f"  â€¢ Parameters: {final_layernorm_params:,}")

print(f"\nğŸ“Š LM Head (usually tied with embedding):")
print(f"  â€¢ Parameters: {lm_head_params:,} (if not tied)")

print(f"\n{'=' * 100}")
print(f"  ğŸ¯ TOTAL MODEL PARAMETERS: {total_params:,}")
print(f"{'=' * 100}")

print(f"\nğŸ“ˆ Model Characteristics:")
print(f"  â€¢ Sparsity: {config.num_experts_per_tok}/{config.num_local_experts} experts activated")
print(f"  â€¢ Sparsity ratio: {config.num_experts_per_tok/config.num_local_experts:.1%}")
print(f"  â€¢ Effective MoE params per token: {(expert_params // config.num_local_experts) * config.num_experts_per_tok + gate_params:,}")
print(f"  â€¢ Attention type: Group Query Attention (GQA)")
print(f"  â€¢ GQA ratio: {config.num_attention_heads // config.num_key_value_heads}:1 (Query:KV)")
print(f"  â€¢ Use QK normalization: {config.use_qk_norm}")

print_section("Key Features")

print("""
âœ¨ Architecture Highlights:

1. ğŸ¯ Group Query Attention (GQA)
   â€¢ Reduces KV cache size while maintaining model quality
   â€¢ Q heads: 48, KV heads: 8 (6:1 ratio)
   â€¢ Per-head Q/K normalization for training stability

2. ğŸ”€ Mixture of Experts (MoE)
   â€¢ 256 experts per layer (massive expert pool)
   â€¢ Top-8 expert routing (sparse activation)
   â€¢ Only ~3.125% of experts active per token
   â€¢ Enables scaling model capacity without proportional compute increase

3. ğŸ¨ SwiGLU Activation
   â€¢ silu(w1(x)) âŠ— w3(x) â†’ w2(x)
   â€¢ Gated activation for better expressiveness
   â€¢ Separate gate and up projections

4. ğŸ”„ RoPE (Rotary Position Embedding)
   â€¢ Theta: 5,000,000 (supports very long contexts)
   â€¢ Max positions: 196,608 tokens (~200K context length!)
   â€¢ Relative positional encoding

5. ğŸ“ RMSNorm
   â€¢ Lightweight normalization (no bias, no mean centering)
   â€¢ Lower computational cost than LayerNorm
   â€¢ eps=1e-6 for numerical stability
""")

print("=" * 100)
print("Visualization complete!")
print("=" * 100)
