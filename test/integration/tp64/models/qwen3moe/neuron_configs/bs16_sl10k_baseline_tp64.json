{
    "tp_degree": 64,
    "attention_dp_degree": 1,
    "cp_degree": 1,
    "batch_size": 16,
    "ctx_batch_size": 1,
    "tkg_batch_size": 16,
    "max_context_length": 10240,
    "seq_len": 10240, 
    "torch_dtype": "float16",
    "fused_qkv": true,
    "is_continuous_batching": true,
    "logical_nc_config": 2,
    "sequence_parallel_enabled": false,
    "qkv_kernel_enabled": false,
    "attn_kernel_enabled": false,
    "strided_context_parallel_kernel_enabled": false
}