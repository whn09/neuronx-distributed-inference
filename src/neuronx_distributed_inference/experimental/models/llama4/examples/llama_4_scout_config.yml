model:
  vocab_size: 202048
  hidden_size: 5120
  n_layers: 48
  n_heads: 40
  n_kv_heads: 8
  head_dim: 128
  intermediate_size: 8192
  intermediate_size_mlp: 16384
  dtype: '${torch_dtype:bfloat16}'  # Use the torch_dtype resolver
  rms_norm_eps: 1e-05
  rope_theta: 500000.0
  pad_token: 200018
  attention_chunk_size: 8192
  interleave_moe_layer_step: 1
  nope_layer_interval: 4
  use_qk_norm: True
  num_local_experts: 16
  num_experts_per_tok: 1
generation:
  batch_size: 2
  sequence_length: 128
  tp_degree: 32